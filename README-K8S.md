# üï∑Ô∏è LinkedIn Spider - Kubernetes Deployment Guide

## Overview

This guide covers deploying LinkedIn Spider as a distributed scraping system on Kubernetes with 1,000 worker pods. The architecture includes:

- **PostgreSQL**: Central database with automatic deduplication
- **Redis**: Message queue for URL distribution
- **1,000 Worker Pods**: Parallel scraping with shared authentication
- **Session Management**: Shared LinkedIn cookies across all workers

---

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Kubernetes Cluster                    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ      ‚îÇ              ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  PostgreSQL  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  Worker Pod  ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  StatefulSet ‚îÇ      ‚îÇ   (x1000)    ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ      ‚îÇ              ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ          ‚ñ≤                     ‚îÇ                        ‚îÇ
‚îÇ          ‚îÇ                     ‚îÇ                        ‚îÇ
‚îÇ          ‚îÇ                     ‚ñº                        ‚îÇ
‚îÇ          ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    Redis     ‚îÇ                ‚îÇ
‚îÇ                         ‚îÇ  (Queue)     ‚îÇ                ‚îÇ
‚îÇ                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Session Cookies (K8s Secret) shared across all workers ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Prerequisites

1. **Minikube** (for local testing) or production Kubernetes cluster
2. **kubectl** configured
3. **Docker** installed
4. **LinkedIn account** for cookie extraction
5. Minimum resources:
   - 8+ CPU cores
   - 16GB+ RAM (for testing with scaled-down workers)
   - 1TB+ RAM (for full 1,000 workers)

---

## Quick Start

### 1. Extract LinkedIn Cookies

First, manually log in to LinkedIn and extract session cookies:

```bash
# Run the cookie extraction wizard
poetry run python -m linkedin_spider.cli.extract_cookies --k8s-secret -o data/linkedin_cookies.pkl -s k8s/linkedin-cookies-secret.yaml

# This will:
# 1. Open a browser
# 2. Let you manually log in (including 2FA)
# 3. Extract and save cookies
# 4. Generate Kubernetes Secret YAML
```

### 2. Build Docker Image

```bash
# Build the image
docker build -t linkedin-spider:latest .

# For minikube, load the image
minikube image load linkedin-spider:latest
```

### 3. Deploy to Kubernetes

```bash
# Apply manifests in order
kubectl apply -f k8s/00-namespace.yaml
kubectl apply -f k8s/01-configmap.yaml
kubectl apply -f k8s/linkedin-cookies-secret.yaml  # Generated by cookie extraction
kubectl apply -f k8s/10-postgres-statefulset.yaml
kubectl apply -f k8s/11-redis.yaml

# Wait for database and Redis to be ready
kubectl wait --for=condition=ready pod -l app=postgres -n linkedin-spider --timeout=300s
kubectl wait --for=condition=ready pod -l app=redis -n linkedin-spider --timeout=300s

# Deploy workers (start with fewer replicas for testing)
kubectl apply -f k8s/20-worker-deployment.yaml

# Apply network policies
kubectl apply -f k8s/30-network-policy.yaml
```

### 4. Push URLs to Queue

```bash
# Create a file with LinkedIn profile URLs (one per line)
cat > profile_urls.txt <<EOF
https://www.linkedin.com/in/example1/
https://www.linkedin.com/in/example2/
https://www.linkedin.com/in/example3/
EOF

# Connect to Redis and push URLs
kubectl run -it --rm redis-client --image=redis:7-alpine -n linkedin-spider -- sh

# Inside the pod:
redis-cli -h redis
ZADD linkedin:urls 0 "https://www.linkedin.com/in/example1/"
ZADD linkedin:urls 0 "https://www.linkedin.com/in/example2/"
# ... or use a script
exit
```

Or use Python to batch push:

```python
from linkedin_spider.queue import get_producer

producer = get_producer()
producer.push_urls_from_file("profile_urls.txt")
print(f"Queue size: {producer.get_queue_size()}")
```

### 5. Monitor Workers

```bash
# Check worker status
kubectl get pods -n linkedin-spider -l app=linkedin-spider-worker

# View logs from a specific worker
kubectl logs -n linkedin-spider <worker-pod-name>

# Follow logs from multiple workers
kubectl logs -f -n linkedin-spider -l app=linkedin-spider-worker --tail=10

# Check database for scraped profiles
kubectl exec -it postgres-0 -n linkedin-spider -- psql -U postgres -d linkedin_spider -c "SELECT COUNT(*) FROM profiles;"

# Check Redis queue size
kubectl run -it --rm redis-client --image=redis:7-alpine -n linkedin-spider -- redis-cli -h redis ZCARD linkedin:urls
```

---

## Scaling

### Adjust Worker Count

```bash
# Scale to 100 workers (for testing)
kubectl scale deployment linkedin-spider-worker -n linkedin-spider --replicas=100

# Scale to full 1,000 workers
kubectl scale deployment linkedin-spider-worker -n linkedin-spider --replicas=1000

# Scale down
kubectl scale deployment linkedin-spider-worker -n linkedin-spider --replicas=10
```

### Resource Considerations

Each worker pod requires:
- **CPU**: 500m request, 1000m limit
- **Memory**: 1Gi request, 2Gi limit

For 1,000 workers:
- **Total CPU**: 500 cores (requests) / 1,000 cores (limits)
- **Total Memory**: 1TB (requests) / 2TB (limits)

---

## Database Queries

### Access Database

```bash
kubectl exec -it postgres-0 -n linkedin-spider -- psql -U postgres -d linkedin_spider
```

### Useful Queries

```sql
-- Total profiles scraped
SELECT COUNT(*) FROM profiles;

-- Profiles by company
SELECT company, COUNT(*) 
FROM profiles 
GROUP BY company 
ORDER BY COUNT(*) DESC 
LIMIT 10;

-- Profiles by location
SELECT location, COUNT(*) 
FROM profiles 
GROUP BY location 
ORDER BY COUNT(*) DESC 
LIMIT 10;

-- Worker performance
SELECT worker_id, COUNT(*) as profiles_scraped
FROM profiles
GROUP BY worker_id
ORDER BY COUNT(*) DESC
LIMIT 20;

-- Failed scrapes
SELECT status, COUNT(*)
FROM scrape_logs
GROUP BY status;

-- Recent scrapes
SELECT url, scraped_at
FROM profiles
ORDER BY scraped_at DESC
LIMIT 10;

-- Export to CSV (from within container)
\copy (SELECT * FROM profiles) TO '/tmp/profiles.csv' WITH CSV HEADER;
```

### Export Data

```bash
# Export profiles to CSV
kubectl exec -it postgres-0 -n linkedin-spider -- psql -U postgres -d linkedin_spider -c "\copy profiles TO STDOUT WITH CSV HEADER" > profiles.csv

# Or use pg_dump for full backup
kubectl exec postgres-0 -n linkedin-spider -- pg_dump -U postgres linkedin_spider > linkedin_spider_backup.sql
```

---

## Troubleshooting

### Workers Not Starting

```bash
# Check worker logs
kubectl logs -n linkedin-spider -l app=linkedin-spider-worker --tail=50

# Check if secrets are mounted correctly
kubectl describe pod -n linkedin-spider <worker-pod-name>

# Verify cookies secret exists
kubectl get secret linkedin-credentials -n linkedin-spider
```

### Database Connection Issues

```bash
# Check PostgreSQL is running
kubectl get pods -n linkedin-spider -l app=postgres

# Check PostgreSQL logs
kubectl logs -n linkedin-spider postgres-0

# Test connection from a worker
kubectl exec -it <worker-pod> -n linkedin-spider -- python -c "from linkedin_spider.db import get_db_client; client = get_db_client(); print(client.get_profile_count())"
```

### Redis Connection Issues

```bash
# Check Redis is running
kubectl get pods -n linkedin-spider -l app=redis

# Test Redis connection
kubectl run -it --rm redis-client --image=redis:7-alpine -n linkedin-spider -- redis-cli -h redis ping

# Check queue stats
kubectl run -it --rm redis-client --image=redis:7-alpine -n linkedin-spider -- redis-cli -h redis INFO
```

### Chrome/Selenium Issues

Workers may fail if Chrome crashes. Check logs for:

```bash
kubectl logs -n linkedin-spider <worker-pod> | grep -i chrome
kubectl logs -n linkedin-spider <worker-pod> | grep -i selenium
```

If Chrome is crashing, you may need to:
1. Increase memory limits
2. Add more `--no-sandbox` flags in browser.py
3. Check `/dev/shm` size

---

## Performance Optimization

### Batch URL Push

For better performance when adding millions of URLs:

```python
from linkedin_spider.queue import get_producer

producer = get_producer()

# Push URLs in large batches
urls = [...] # Load from database, CSV, etc.
batch_size = 10000

for i in range(0, len(urls), batch_size):
    batch = urls[i:i+batch_size]
    producer.push_urls_batch(batch)
    print(f"Pushed {i+batch_size} URLs")
```

### Database Optimization

```sql
-- Add indexes for faster queries (already included in schema)
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_company ON profiles(company);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_location ON profiles(location);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_scraped_at ON profiles(scraped_at);

-- Vacuum analyze periodically
VACUUM ANALYZE profiles;
VACUUM ANALYZE scrape_logs;
```

### Redis Optimization

For very large queues (millions of URLs), consider:

```yaml
# In k8s/11-redis.yaml, add memory optimization
command:
  - redis-server
  - --appendonly
  - "yes"
  - --maxmemory
  - "4gb"
  - --maxmemory-policy
  - "noeviction"
```

---

## Cleanup

```bash
# Delete all resources
kubectl delete namespace linkedin-spider

# Or delete individually
kubectl delete -f k8s/20-worker-deployment.yaml
kubectl delete -f k8s/11-redis.yaml
kubectl delete -f k8s/10-postgres-statefulset.yaml
kubectl delete -f k8s/02-secret-template.yaml
kubectl delete -f k8s/01-configmap.yaml
kubectl delete -f k8s/00-namespace.yaml

# Delete persistent volumes
kubectl delete pvc -n linkedin-spider --all
```

---

## Security Best Practices

1. **Cookie Rotation**: Rotate LinkedIn cookies every 7-14 days
2. **Secrets Management**: Never commit `linkedin-cookies-secret.yaml` to git
3. **Network Policies**: Keep network policies enabled
4. **Resource Limits**: Always set resource limits to prevent runaway pods
5. **RBAC**: Use Kubernetes RBAC to restrict access
6. **Database Credentials**: Use strong passwords, rotate regularly

---

## Cost Considerations

Running 1,000 workers continuously:
- **Cloud Cost**: ~$500-2,000/day depending on provider
- **Recommendation**: Start with 10-50 workers, scale as needed
- **Spot Instances**: Use spot/preemptible instances for 60-80% savings

---

## Next Steps

1. Test with 10 workers first
2. Monitor for 30 minutes
3. Gradually scale up: 10 ‚Üí 50 ‚Üí 100 ‚Üí 500 ‚Üí 1,000
4. Monitor database performance
5. Adjust delays in `config.yaml` to avoid LinkedIn rate limits
6. Set up monitoring (Prometheus/Grafana) for production

---

## Support

For issues or questions:
- Check logs first: `kubectl logs -n linkedin-spider <pod-name>`
- Review database logs for connection issues
- Check Redis for queue backlog
- Monitor resource usage: `kubectl top pods -n linkedin-spider`

Happy scraping! üï∑Ô∏è
